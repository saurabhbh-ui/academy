payload:

[
  {
    "fileType": ".pdf",
    "content": "string",
    "figures": [
      {
        "id": "string",
        "b64image": "string",
        "caption": "string",
        "footnote": "string",
        "pageNumber": 0
      }
    ]
  }
]



Schemas: 
import enum
import logging
from typing import List, Optional, Union

from humps import camelize
from openai.types.chat import ChatCompletionRole
from pydantic import BaseModel as PydanticBaseModel
from pydantic import ConfigDict, Field, model_validator
from typing_extensions import Literal, TypeAlias

logger = logging.getLogger(__name__)


class BaseModel(PydanticBaseModel):
    model_config = ConfigDict(alias_generator=camelize, populate_by_name=True, use_enum_values=True)


# ---------------------------------  Enums ---------------------------------- #
# --------------------------------------------------------------------------- #


@enum.unique
class TutorialSuiteStage(str, enum.Enum):
    outline = "outline"
    brief = "brief"
    test_yourself = "test_yourself"
    connect = "connect"
    exsum = "exsum"


@enum.unique
class BriefCount(str, enum.Enum):
    one = "one"
    two = "two"
    three = "three"
    four = "four"
    five = "five"

    __NUMERIC_MAP__ = {
        "one": 1,
        "two": 2,
        "three": 3,
        "four": 4,
        "five": 5,
    }

    def to_numeric(self):
        return self.__NUMERIC_MAP__[self.value]


@enum.unique
class Tone(str, enum.Enum):
    basic = "basic"
    intermediate = "intermediate"
    advanced = "advanced"


@enum.unique
class SpecialisedTone(str, enum.Enum):
    simple_and_clear = "simple_and_clear"
    professional_and_concise = "professional_and_concise"
    engaging_and_conversational = "engaging_and_conversational"
    explanatory_and_visual = "explanatory_and_visual"
    scenario_based_and_practical = "scenario_based_and_practical"
    technical_and_analytical = "technical_and_analytical"
    step_by_step_instructional = "step_by_step_instructional"
    comparative_and_contextual = "comparative_and_contextual"
    encouraging_and_motivational = "encouraging_and_motivational"
    interactive_and_question_oriented = "interactive_and_question_oriented"

    __PROMPT_MAP__ = {
        "simple_and_clear": "The tone of each brief should emphasize plain language, avoid jargon, and explain complex concept with relatable examples. The target audience is entry-level.",
        "professional_and_concise": "Each brief should use a formal and precise approach with minimal elaboration. The target audience is technical specialists.",
        "engaging_and_conversational": "Each brief should create a welcoming atmosphere and encourages participation.",
        "explanatory_and_visual": "Each brief should break down concepts into manageable parts to bridge knowledge gaps.",
        "scenario_based_and_practical": "The breifs should focus on real-world applications to enhance understanding.",
        "technical_and_analytical": "In-depth explanations with data and advanced terminology.",
        "step_by_step_instructional": "Guides through processes or calculations.",
        "comparative_and_contextual": "Each brief should explain international standards in comparison to regional practices or historical trends.",
        "encouraging_and_motivational": "Each brief should emphasize the importance of the role of the trainee and build their confidence.",
        "interactive_and_question_oriented": "Each brief should engage the audience by prompting critical thinking and self-reflection.",
    }

    def to_prompt_statement(self) -> str:
        return self.__PROMPT_MAP__[self.value]


@enum.unique
class NewLevel(str, enum.Enum):
    beginner = "Beginner"
    intermediate = "Intermediate"
    advanced = "Advanced"


@enum.unique
class NewLength(str, enum.Enum):
    shortest = "shortest"
    shorter = "shorter"
    longer = "longer"
    longest = "longest"

    __PROMPT_MAP__ = {
        "shortest": "much shorter than it currently is",
        "shorter": "slightly shorter than it currently is",
        "longer": "slightly longer than it currently is",
        "longest": "much longer than it currently is",
    }

    def to_prompt_value(self) -> str:
        return self.__PROMPT_MAP__[self.value]


@enum.unique
class EventType(str, enum.Enum):
    on_rewrite_block = "on_rewrite_block"
    on_rewrite_artifact = "on_rewrite_artifact"
    on_chat_response = "on_chat_response"
    on_progress_update = "on_progress_update"


# ----------------------- Document & Artifact Models ------------------------ #
# --------------------------------------------------------------------------- #


class Figure(BaseModel):
    id: str
    b64image: str
    caption: Optional[str]
    footnote: Optional[str]
    page_number: int


class SourceDocument(BaseModel):
    file_type: str = ".pdf"
    content: str
    figures: list[Figure]


class TutorialConfiguration(BaseModel):
    model_config = ConfigDict(alias_generator=camelize, populate_by_name=True, use_enum_values=False)

    title: Optional[str]
    tone: Optional[SpecialisedTone]
    brief_count: BriefCount
    sections_to_highlight: Optional[str] = None
    sections_to_exclude: Optional[str] = None


class Artifact(BaseModel):
    content: str


class ArtifactChunk(BaseModel):
    block: str
    selection: str


class DocName(BaseModel):
    header_title: str = Field(default="No title", description="Document name or should be main title of the document")


# --------------------------- Event Stream Models --------------------------- #
# --------------------------------------------------------------------------- #


class StreamingEvent(BaseModel):
    event: EventType
    chunk: str


# ------------------------------- Brief Models ------------------------------ #
# --------------------------------------------------------------------------- #


class BriefInstructions(BaseModel):
    title: str = Field(description="The title of the brief, found in the '##' heading.")
    objectives: str = Field(
        description='The objectives of the brief, found in the paragraph starting with "**Objectives**" right after the heading. This is typically a single line, but could also be on several lines or a short bulleted list.'
    )
    overview: str = Field(
        description='The overview of the brief, found in the paragraph starting with "**Overview**", typically after the objectives.'
    )
    content: str = Field(
        description='The content of the brief, found in the paragraph starting with "**Content**", typically after the overview. This is is typically a long bulleted list.'
    )


class ParsedOutline(BaseModel):
    briefs: list[BriefInstructions] = Field(..., min_length=1)


# ----------------------- Refinement Request Models ------------------------- #
# --------------------------------------------------------------------------- #

# Create an extended role type to handle the error messages from the backend
ExtendedChatCompletionRole: TypeAlias = Union[ChatCompletionRole, Literal["error"]]


class ChatMessage(BaseModel):
    role: ExtendedChatCompletionRole
    content: str


class BaseRefinementRequest(BaseModel):
    messages: list[ChatMessage]
    artifact: str
    source: Union[SourceDocument, list[SourceDocument]]  # Updated to support both single and multiple documents

    @model_validator(mode="before")
    @classmethod
    def filter_error_messages(cls, data: dict[str, any]) -> dict[str, any]:
        if "messages" in data:
            len_before = len(data["messages"])

            # Filter out messages with role "error"
            data["messages"] = [
                msg for msg in data["messages"] if not (isinstance(msg, dict) and msg.get("role") == "error")
            ]

            len_after = len(data["messages"])
            if len_after < len_before:
                logger.info(f"Filtered {len_before - len_after} error messages")
        return data


class ChatCompletionRequest(BaseRefinementRequest):
    stage: TutorialSuiteStage
    config: Optional[TutorialConfiguration] = None
    brief_instructions: Optional[BriefInstructions] = None
    # source: Union[SourceDocument, list[SourceDocument]]  # Updated to support multiple documents

    @model_validator(mode="after")
    def validate_stage_requirements(self) -> "ChatCompletionRequest":
        """
        The optional `config` and `brief_instructions` are required depending on the stage.
        This validator checks that they are provided for each possible stage.
        """
        match self.stage:
            case TutorialSuiteStage.outline:
                # For outline stage, config must be provided
                if self.config is None:
                    raise ValueError("config is required for the 'outline' stage")

            # For brief stage, brief_instructions must be provided
            case TutorialSuiteStage.brief:
                if self.brief_instructions is None:
                    raise ValueError("brief_instructions is required for the 'brief' stage")

            case _:
                if (self.brief_instructions is not None) or (self.config is not None):
                    stage = self.stage
                    raise ValueError(f"brief_instructions and config should NOT be provided for the '{stage}' stage")

        return self


class AdjusLengthRequest(BaseRefinementRequest):
    model_config = ConfigDict(alias_generator=camelize, populate_by_name=True, use_enum_values=False)
    new_length: NewLength


class AdjustLevelRequest(BaseRefinementRequest):
    model_config = ConfigDict(alias_generator=camelize, populate_by_name=True, use_enum_values=False)
    new_level: NewLevel


class UpdateSelectionRequest(BaseModel):
    messages: list[ChatMessage]
    source: Union[SourceDocument, list[SourceDocument]]
    artifact_chunk: ArtifactChunk
    query: str


# ------------------------- Complex Response Models-------------------------- #
# --------------------------------------------------------------------------- #


class ArtifactWithResponse(Artifact):
    response: Optional[ChatMessage]


# ------------------------------ Connect Models ----------------------------- #
# --------------------------------------------------------------------------- #


class ConnectConfig(BaseModel):
    role: Optional[str] = Field(
        default=None,
        description="Professional role the learner assumes in the scenario. Can specify 'other' or any custom role.",
    )
    department: Optional[str] = Field(
        default=None,
        description="Functional department where the scenario is set. Can specify 'other' or any custom department.",
    )
    country_type: Optional[List[str]] = Field(
        default=None, description="Classification of the country. Can specify 'other' or any custom type."
    )
    authority_type: Optional[List[str]] = Field(
        default=None,
        description="Type of regulatory or supervisory authority involved. Can specify 'other' or any custom type.",
    )
    financial_institution: Optional[List[str]] = Field(
        default=None,
        description="The kind of financial institution featured. Can specify 'other' or any custom institution.",
    )
    artefacts: Optional[List[str]] = Field(
        default=None, description="Scenario information sources. Can specify 'other' or any custom sources."
    )
    characters: Optional[List[str]] = Field(
        default=None, description="Fictional characters in the scenario. Can specify 'other' or any custom characters."
    )
    scenario_description: Optional[str] = Field(
        default=None,
        description="A brief narrative outlining the context or background of the scenario. It sets the stage for the learner by describing the situation they will address.",
    )
    keypoints: Optional[List[str]] = Field(default=None, description="Key tasks the learner must perform.")
    task_examples: Optional[str] = Field(default=None, description="Example tasks shaping the scenario.")
    questions: Optional[List[Literal["Fill-in-the-Blank", "Multiple Choice Question", "True/False", "Yes/No"]]] = Field(
        default=None,
        description="Types of questions to include. Possible values are: Fill-in-the-Blank, Multiple Choice Question, True/False.",
    )


class ConnectRequest(BaseModel):
    briefs: list[Artifact]
    parsed_outline: ParsedOutline
    config: ConnectConfig


######## reviewer models


@enum.unique
class ReviewModule(str, enum.Enum):
    brief = "brief"
    test_yourself = "test_yourself"
    general = "general"  # For other modules without specific rules


class ReviewRequest(BaseModel):
    """Request model for content review"""

    content: str = Field(..., description="The content to be reviewed", min_length=1)
    module: ReviewModule = Field(
        default=ReviewModule.general, description="The module type to determine which validation rules to apply"
    )
    source: Optional[Union[SourceDocument, List[SourceDocument]]] = Field(
        default=None, description="Source document(s) for coverage validation (optional, recommended for brief module)"
    )



### code for exsum: 

## C:\Users\sa007769\Downloads\data-fsi-aicademy-api\app\api\exsum\router.py

import logging

import openai
from fastapi import APIRouter, HTTPException, Request

from app.api.schemas import ArtifactWithResponse, ChatMessage, SourceDocument
from app.core.exsum_generation import ExsumPromptTemplate, generate_executive_summary
from app.core.followup import GenerationFollowupPromptTemplate, write_followup_message

router = APIRouter(prefix="/api/exsum")
logger = logging.getLogger(__name__)


@router.post("/generate-exsum", summary="Generate Executive Summary")
async def generate_exsum_endpoint(request: Request, source: list[SourceDocument]) -> ArtifactWithResponse:
    """
    Generate Executive Summary.
    This API generates **Executive summary from Source**.
    """
    try:
        prompts: ExsumPromptTemplate = request.app.prompt_registry["exsum"]
        artifact: str = generate_executive_summary(prompts=prompts, source=source)

        followup_prompts: GenerationFollowupPromptTemplate = request.app.prompt_registry["followup"]
        followup_content: str = write_followup_message(
            prompts=followup_prompts, stage="Executive Summary", artifact=artifact
        )
        followup_message = ChatMessage(role="assistant", content=followup_content)

        return ArtifactWithResponse(content=artifact, response=followup_message)

    except openai.APIStatusError as err:
        raise HTTPException(status_code=err.status_code, detail=err.message)
    except openai.OpenAIError as err:
        raise HTTPException(status_code=500, detail=f"OpenAIError of type {type(err)} was caught: {str(err)}")



## C:\Users\sa007769\Downloads\data-fsi-aicademy-api\app\core\exsum_generation\chains.py
import copy
import logging

from langchain_core.prompts import ChatPromptTemplate

from app.api.schemas import DocName, SourceDocument
from app.core.openai_ops import get_llm

from .prompts import ExsumPromptTemplate
from .suggest_exsum_figures import suggest_executive_summary_figures
from .write_exsum import write_exsum_with_figures

logger = logging.getLogger(__name__)


def generate_executive_summary(prompts: ExsumPromptTemplate, source: list[SourceDocument]) -> str:
    """
    Generate an executive summary from the provided source document.

    This function uses the provided prompts to suggest figures and write an executive summary
    based on the source document.

    Parameters:
    - prompts (ExsumPromptTemplate): The template containing prompts for generating the executive summary.
    - source (SourceDocument): The source document from which the executive summary is to be generated.

    Returns:
    - str: The generated executive summary content, or None if an error occurs during generation.
    """

    model = get_llm()

    sources = copy.deepcopy(source)

    # Merge the content of all source documents, separating them with headers
    merged_content = ""
    for i, source in enumerate(sources):
        header_template = ChatPromptTemplate.from_template(prompts.file_header.prompt.template)
        doc_model = model.with_structured_output(DocName)
        header_chain = header_template | doc_model
        headers = header_chain.invoke({"text": source.content})
        logger.info(f"Document {i + 1}: {headers.header_title}")

        # Append the header and content to the merged content
        text_header = f"########################### File: {i + 1}, Document Name: {headers.header_title} #########################\n\n{source.content}\n\n"
        merged_content += text_header

    # Create a merged SourceDocument
    merged_source = SourceDocument(
        file_type=".pdf", content=merged_content, figures=[figure for source in sources for figure in source.figures]
    )

    suggested_figures = suggest_executive_summary_figures(prompts, merged_source)
    exsum_content = write_exsum_with_figures(prompts=prompts, source=merged_source, suggested_figures=suggested_figures)
    return exsum_content


## C:\Users\sa007769\Downloads\data-fsi-aicademy-api\app\core\exsum_generation\prompts.py
import logging
from dataclasses import dataclass

from langchain_core.messages import HumanMessage
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate

from app.api.schemas import Figure, SourceDocument

logger = logging.getLogger(__name__)


@dataclass
class ExsumPromptTemplate:
    system: SystemMessagePromptTemplate
    file: HumanMessagePromptTemplate
    file_header: HumanMessagePromptTemplate
    figure_caption: HumanMessagePromptTemplate
    suggest_figure_system: HumanMessagePromptTemplate

    def build_suggest_figures_prompt_template(self, source: SourceDocument) -> ChatPromptTemplate:
        prompt_template = ChatPromptTemplate([self.suggest_figure_system])
        # Add all figures in has a new message in the prompt template
        for fig in source.figures:
            try:
                prompt_template += HumanMessage(
                    content=[
                        {
                            "type": "text",
                            "text": f'The caption of this image is: "{fig.caption}". Its id is "{fig.id}".',
                        },
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{fig.b64image}"}},
                    ]
                )
            except Exception as e:
                # Ignore figure in case of error and continue
                logger.error(f"Error processing figure ID {fig.id}: {e}", extra={"figure": fig.model_dump()})
                continue
        return prompt_template

    def build_write_exsum_prompt_template(self, figures_data: list[Figure]) -> ChatPromptTemplate:
        """
        Build messages for figures to be included in the executive summary.

        This function formats the captions and image URLs for each figure using
        the provided prompts and returns a list of HumanMessage objects.

        Parameters:
        - figures_data (list[dict]): A list of dictionaries containing figure data.

        Returns:
        - list[HumanMessage]: A list of formatted HumanMessage objects for figures.
        """
        # Validate input variables because `.format` does not
        if not sorted(self.figure_caption.input_variables) == ["caption", "id"]:
            raise KeyError("Misaligned input variables found in prompt `exec_summary_figure_prompt`")

        prompt_template = ChatPromptTemplate([self.system, self.file])
        for fig in figures_data:
            try:
                # Multimodal messages are not well handled with ChatPromptTemplate, so we render the template here
                # for the figure messages
                content: HumanMessage = self.figure_caption.format(id=fig.id, caption=fig.caption).content
                prompt_template += HumanMessage(
                    content=[
                        {"type": "text", "text": content},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{fig.b64image}"}},
                    ]
                )
            except Exception as e:
                logger.error(f"Error building figure message for figure ID : {e}", exc_info=False)
                continue
        return prompt_template


## C:\Users\sa007769\Downloads\data-fsi-aicademy-api\app\core\exsum_generation\suggest_exsum_figures.py
import logging
import re

from app.api.schemas import Figure, SourceDocument
from app.core.openai_ops import get_llm

from .prompts import ExsumPromptTemplate

logger = logging.getLogger(__name__)


def _extract_ids(text) -> list[str]:
    """
    Extracts IDs from the given text using a regular expression pattern.
    This function searches for all substrings enclosed within double square brackets [[ ]].

    Parameters:
    - text (str): The text from which to extract IDs.

    Returns:
    - List[str]: A list of extracted IDs.
    """

    pattern = r"\[\[(.*?)\]\]"
    matches = re.findall(pattern, text)
    matches = [m.strip() for m in matches]  # avoid issue with spaces added by LLM in id
    return matches


def suggest_executive_summary_figures(prompts: ExsumPromptTemplate, source: SourceDocument) -> list[Figure]:
    """
    Suggest figures for the executive summary based on the source document.

    This function utilizes a language model to analyze the figures in the source document
    and suggests which figures should be included in the executive summary.

    Parameters:
    - prompts (ExsumPromptTemplate): The prompt template for suggesting figures.
    - source_doc (SourceDocument): The source document containing figures to be analyzed.

    Returns:
    - List[Figure]: A list of figures suggested for inclusion in the executive summary.
    """
    if len(source.figures) == 0:
        return []
    else:
        model = get_llm()
        prompt_template = prompts.build_suggest_figures_prompt_template(source=source)
        chain = prompt_template | model
        response = chain.invoke(input={})  # no inputs, variables, only multimodal messages
        suggested_ids = _extract_ids(response.content)
        suggested_figures_data = [fig for fig in source.figures if fig.id in suggested_ids]
        logger.info(
            f"Chose the following figures for the exsum: {suggested_ids}",
            extra={"response": response.model_dump(), "suggested_ids": suggested_ids},
        )
        return suggested_figures_data


## C:\Users\sa007769\Downloads\data-fsi-aicademy-api\app\core\exsum_generation\write_exsum.py
import logging

from langchain_core.prompts import ChatPromptTemplate

from app.api.schemas import Figure, SourceDocument
from app.core.openai_ops import get_llm

from .prompts import ExsumPromptTemplate

logger = logging.getLogger(__name__)


def write_exsum_with_figures(
    prompts: ExsumPromptTemplate, source: SourceDocument, suggested_figures: list[Figure]
) -> str:
    """
    Write an executive summary including specified figures.

    This function utilizes a language model to generate an executive summary
    based on the provided source document and suggested figures.

    Parameters:
    - exsum_prompt (ExsumPromptTemplate): The template containing prompts for the executive summary.
    - source (SourceDocument): The source document to base the executive summary on.
    - suggested_figures (list[dict]): A list of suggested figures to include in the summary.

    Returns:
    - str: the generated executive summary artifact
    """
    model = get_llm()
    prompt_template: ChatPromptTemplate = prompts.build_write_exsum_prompt_template(figures_data=suggested_figures)
    chain = prompt_template | model
    response = chain.invoke({"file_type": source.file_type, "parsed_content": source.content})
    artifact = response.content
    return artifact
